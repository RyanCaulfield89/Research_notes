\documentclass[12pt,letterpaper]{article}

\usepackage{amssymb,amsmath,amsthm,indentfirst}
%Amsmath command that numbers equation by section
\numberwithin{equation}{section}

\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{color}

%Has all the bracket notation and vector stuff 
\usepackage{physics}

% general definitions
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\reals}{\mathbb{R}}

\title{Research Notes}
\author{Ryan Caulfield}
\date{}
\begin{document}

\maketitle
\newpage

\maketitle
\tableofcontents

\newpage
\section{Potentials in QM}
\subsection{Local vs Non-Local Potentials}
Often, our first examples in quantum mechanics are systems with local potentials in position space. To be specific, we represent our wave function $\ket{\Psi}$ as a complex valued function of $\va{r}$ and $t$, $\Psi(\va{r},t)$, and our potential is a function of just the spacial coordinates, $V(\va{r})$. Then the Schrödinger equation becomes a separable partial differential equation for the unknown function $\Psi(\va{r},t)$. The Schrödinger equation becomes the following.
\beq
i\hbar\pdv{t} \Psi(\va{r},t) = -\frac{\hbar^2}{2m} \laplacian \Psi(\va{r},t) + V(x)\Psi(\va{r},t)
\eeq
In this case, the operator $\hat{V}$ applied to $\Psi(\va{r},t)$ just yields $V(\va{r})\Psi(\va{r})$. The result only depends on the wave function $\Psi(\va{r},t)$ at the point $\va{r}$. We call this a local potential, as it only depends on the wave function locally at the point $\va{r}$. 
\newline \indent
A more abstract way to define this is to say that a potential $\hat{V}$ is local with respect to a basis if it is diagonal in that basis, i.e. $\matrixel{x}{\hat{V}}{x'} = 0$ if $x\neq x'$. This need not always be the case. We could have a potential such that  $\hat{V}\Psi(\va{r},t) = \int V(\va{r},\va{r'})\Psi(\va{r'},t)d^3\va{r'}$. In this case, the Schrödinger equation becomes the following.
\beq
i\hbar\pdv{t} \Psi(\va{r},t) = -\frac{\hbar^2}{2m} \laplacian \Psi(\va{r},t) + \int V(\va{r},\va{r'})\Psi(\va{r'},t)d^3\va{r'}
\eeq
This is now an integro-differential equation, which is more complicated to solve. One method of solving this is to discretize the system and solve it numerically. This is practical in one spacial dimension or when the potential only depends on the magnitude of the spacial coordinate. When this is true, it can be recast as a matrix equation. Unfortunately, the matrix will be dense, making these methods slow.
\subsection{A Numerical Example of a Non-Local Potential}
\textcolor{red}{Find bound states of a non-local nuclear potential. Include SO-coupling and coulomb repulsion. Use wood-saxon potentials like in Bertulani chapter 10 and the paper by Cho et al.} 

\section{Scattering}
\textcolor{red}{Brief intro}

\subsection{Scattering Amplitude}
\textcolor{red}{Define scattering amplitude}

\subsection{Partial Waves}
\textcolor{red}{Explain partial waves and phase shifts. Show how to reconstruct scattering amplitude from phase shifts.}

\subsection{Scattering Matrices}
\textcolor{red}{Define S, T and R/K matrices. Show how to solve for them and get phase shifts.}

\section{Nuclear Reactions}
\textcolor{red}{Basics about nuclear reactions}

\subsection{Nuclear Optical Potential}
Suppose we have a single nucleon scattering off of a nucleus with N nucleons. We can describe this by a Hamiltonian $\boldsymbol{H}$.

\begin{center}
$\boldsymbol{H} = \boldsymbol{T_0} + \boldsymbol{V}(r_0, r_1, \dots, r_N) + \boldsymbol{H_{nuc}}(r_1, r_2, \dots, r_N)$
\end{center}

$\boldsymbol{T_0}$ is the kinetic operator for the projectile, $\boldsymbol{H_{nuc}}$ is the Hamiltonian which describes the bound particles in the nucleus, and $\boldsymbol{V}$ is the potential between the projectile and all of the particles in the nucleus. Let $\{\phi_i\}_{i=1}^N$ be the eigenstates of $\boldsymbol{H_{nuc}}$ with energies $\{\epsilon_i\}$. We want to solve for the eigenstates of the full Hamiltonian, so let's expand the eigenfunction $\Psi_E$ in the complete basis $\{\phi_i\}$.

\begin{center}
$\Psi_E(r_0,r_1,\dots) = \sum_{j=0}^{N} u_j(r_0)\phi_j(r_1,r_2,\dots)$
\end{center}

\noindent Let's substitute this expansion into the Schrödinger equation, $\boldsymbol{H} \Psi_E = E \Psi_E$. 

\begin{center}
$\sum_{j=0}^{N} \boldsymbol{T_0}u_j\phi_j + \boldsymbol{V}u_j\phi_j + \boldsymbol{H_{nuc}}u_j\phi_j = E \sum_{j=0}^{N} u_j\phi_j$
\end{center}

\noindent Now, we use the fact that $\boldsymbol{H_{nuc}}u_j\phi_j = u_j\boldsymbol{H_{nuc}}\phi_j = \epsilon_j u_j\phi_j$

\begin{center}
$\sum_{j=0}^{N} (\boldsymbol{T_0} + \boldsymbol{V} + \epsilon_j) u_j\phi_j = E \sum_{j=0}^{N} u_j\phi_j$
\end{center}

\noindent Since the $\phi_j$'s are orthogonal, we can multiply both sides by $\phi_i$ on both sides and integrate over the variables $(r_1,r_2,\dots)$ to obtain the following.
\newline
Note: $\matrixel{\phi_i}{\boldsymbol{V}u_j}{\phi_j} = \matrixel{\phi_i}{\boldsymbol{V}}{\phi_j}u_j = V_{ij}u_j$ 

\begin{center}
$\sum_{j=0}^{N}(\boldsymbol{T_0}\delta_{ij} + V_{ij} + \epsilon_j\delta_{ij}) u_i = E \sum_{j=0}^{N}u_j\delta_{ij}$
\end{center}
\begin{center}
$(\boldsymbol{T_0} + \epsilon_i) u_i +  \sum_{j=0}^{N}V_{ij}u_j = E u_i$
\end{center}

\noindent For simplification, we introduce the following notation.

\begin{center}
$\boldsymbol{\Phi} =
\begin{pmatrix}
u_1 \\ u_2 \\ \vdots
\end{pmatrix}$
\end{center}

\begin{center}
$\boldsymbol{V_0} =
\begin{pmatrix}
V_{01} & V_{02} & \dots
\end{pmatrix}$
\end{center}

\begin{center}
$H_{ij} = \boldsymbol{T_0}\delta_{ij} + V_{ij} + \epsilon_j\delta_{ij}$
\end{center}

\noindent Looking at the  $i = 0$ case and $i \neq 0$ case, we have the following two equations.

\beq
(\boldsymbol{T_0} + V_{00} + \epsilon_0 - E)u_0 = -\boldsymbol{V_0}\boldsymbol{\Phi}
\eeq

\beq
(\boldsymbol{H} - E)\boldsymbol{\Phi} = -\boldsymbol{V_0^\dagger}u_0
\eeq

\noindent We can eliminate $\boldsymbol{\Phi}$ in this expression and get the following equation for $u_0$.

\beq
(\boldsymbol{T_0} + V_{00}(r_0) + \boldsymbol{V_0} \frac{1}{E + i\eta - \boldsymbol{H}} \boldsymbol{V_0^\dagger})u_0(r_0) = (E - \epsilon_0)u_0(r_0)
\eeq

We now have a single variable time-independent Schrödinger equation for the unknown $u_0(r_0)$, with a modified potential which we will call the optical potential, $V_{opt}$.

\beq
V_{opt}(r_0) = V_{00}(r_0) + \boldsymbol{V_0} \frac{1}{E + i\eta - \boldsymbol{H}} \boldsymbol{V_0^\dagger}
\eeq

This is, in general, an energy dependent, complex and non-local potential when working with outgoing spherical wave boundary conditions. This is not a simple quantity to calculate for a given $\boldsymbol{V}$, and the non-locality of $V_{opt}$ makes it difficult to solve equation $(3.3)$. 

\subsection{A Phenomenological Optical Potential}
\textcolor{red}{This should give an explanation like whats in bertulani.}

\subsection{A Numerical Example of Scattering With an Optical Potential}
\textcolor{red}{This should involve code and plots.}

\subsection{A Simple Model of a Two Level Nuclei}
Here we will explore a simple system in which we can find $\boldsymbol{V_{opt}}$ and $\{u_i\}$ exactly. Suppose our nucleus was just a simple two level system. We can describe $\boldsymbol{H_{nuc}}$ and $\{\phi_i\}$ as follows.

\begin{center}
$\boldsymbol{H_{nuc}} = 
\begin{pmatrix}
\epsilon_0 & 0 \\ 
0 & \epsilon_1
\end{pmatrix}$
\end{center}

\begin{center}
$\phi_0 = 
\begin{pmatrix}
1 \\ 0
\end{pmatrix}$
\end{center}

\begin{center}
$\phi_1 = 
\begin{pmatrix}
0 \\ 1
\end{pmatrix}$
\end{center}

\noindent We will work in the basis of ${\phi_0,\phi_1}$ and the momentum basis for the scatterer $p$. Let's imagine the simplest potential we can which couples the two states $\phi_0$ and $\phi_1$.

\begin{center}
$\boldsymbol{V} =
\begin{pmatrix}
0 & V_{0} \\ 
V_{0}^* & 0
\end{pmatrix}
$
\end{center}

\textcolor{red}{From scratch}

\noindent Let's expand $\Psi_E$ in the basis $\{\phi_i\}$.

\begin{center}
$\Psi_E(p) = u_0(p)\begin{pmatrix}
1 \\ 0
\end{pmatrix}
+ u_1(p)\begin{pmatrix}
0 \\ 1
\end{pmatrix}
=
\begin{pmatrix}
u_0(p) \\ u_1(p)
\end{pmatrix}$
\end{center}

\noindent Substituting this expansion into the Schrödinger equation, we get the following.

\begin{center}
$\boldsymbol{T_0}
\begin{pmatrix}
u_0(p) \\ u_1(p)
\end{pmatrix}
+
\begin{pmatrix}
0 & V_{0} \\ 
V_{0}^* & 0
\end{pmatrix}
\begin{pmatrix}
u_0(p) \\ u_1(p)
\end{pmatrix}
+
\begin{pmatrix}
\epsilon_0 & 0 \\ 
0 & \epsilon_1
\end{pmatrix}
\begin{pmatrix}
u_0(p) \\ u_1(p)
\end{pmatrix}
=
E
\begin{pmatrix}
u_0(p) \\ u_1(p)
\end{pmatrix}$
\end{center}

\begin{center}
$\begin{pmatrix}
\boldsymbol{T_0}u_0(p)+V_{0}u_1(p)+\epsilon_0u_0(p)=Eu_0(p) \\ 
\boldsymbol{T_0}u_1(p)+V_{0}^*u_0(p)+\epsilon_1u_1(p)=Eu_1(p)
\end{pmatrix}$
\end{center}

\noindent Solving the second equation for $u_1$ and substituting into the second equation we get the following.

\begin{center}
$(\boldsymbol{T_0}+V_{0}\frac{1}{E + i\eta -\epsilon_1-\boldsymbol{T_0}}V_{0}^*+\epsilon_0)u_0(p)=Eu_0(p)$
\end{center}

\begin{center}
$(p^2+\frac{\abs{V_{0}}^2}{E + i\eta -\epsilon_1-p^2} )u_0(p)=(E - \epsilon_0)u_0(p)$
\end{center}

\begin{center}
$V_{opt}(p) = \frac{\abs{V_{0}}^2}{E + i\eta - \epsilon_1 - p^2}$
\end{center}

\textcolor{red}{Just evaluate 3.4}

\noindent In the notation of equation $3.4$ we have the following.
\begin{center}
$\boldsymbol{V_0} = \matrixel{0}{\boldsymbol{V}}{1} = V_{0}$
\end{center}
\begin{center}
$\boldsymbol{V_0}^\dagger = \matrixel{0}{\boldsymbol{V}}{1}^* = V_{0}^*$
\end{center}
\begin{center}
$\boldsymbol{H} = H_{11} = \boldsymbol{T_0} + \boldsymbol{V_{11}} + \epsilon_1 = p^2 + \epsilon_1$
\end{center}
\begin{center}
$V_{00} = \matrixel{0}{\boldsymbol{V}}{0} = 0$
\end{center}

\noindent We can evaluate equation $3.4$.
\begin{center}
$V_{opt}(p) = \frac{\abs{V_{0}}^2}{E + i\eta - \epsilon_1 - p^2}$
\end{center}

\newpage
\appendix

\section{Interpolation}
To be able to do Calculus, we must be able to represent a function numerically. We can of course define a function by a rule, but this is not useful when we are trying to represent a function that we do not know explicitly. Often, we only know that a function satisfies some differential equation or recurrence relation, and evaluating it at a point may be difficult. To solve this, we represent a function by sampling the function at a finite number of points. We can then evaluate the function at other points using interpolation.

\subsection{Global Interpolation by Polynomials}
First, we can sample the function at a sequence of points, and approximate it by the unique polynomial which agrees with the function at each points. Let's consider a function $f(x) : [a,b] \rightarrow \reals$ Let $x_i$ be a sequence of points in $[a,b]$ such that $x_0 = a$ and $x_N = b$. Now, we can approximate $f(x)$ by the unique polynomial which agrees with $f(x)$ at each point $x_i$. To find such a polynomial, we need to find coefficients $a_j$ such that for each $i$,
\begin{center}
$a_0 + a_1 x_i + a_2 x_i^2 + ... + a_N x_i^N = f(x_i)$
\end{center}

\indent We can convert this to a matrix equation $\boldsymbol{X}\va{a} = \va{f}$, where $\va{a}$ is the vector of coefficients, $\va{f}$ is the vector of function values such that $f_i = f(x_i)$, and $X_{ij} = x_i^j$ Now, we can find the coefficients easily by solving the matrix equation. The solution is given by 
\begin{center}
$\va{a} = \boldsymbol{X}^{-1} \va{f}$
\end{center}
With the coefficients, we can evaluate $f(x)$ at any $x \in [a,b]$ by
\begin{center}
$f(x) = \sum_{i=0}^{N} a_i x^i$ 
\end{center}

\indent As constructed, this method of interpolation should approximate an Nth degree polynomial exactly. But, how accurately does it approximate non-polynomial functions? The Stone-Weierstrass theorem tells us that every continuous function can be uniformly approximated as a polynomial. One would assume that as the number of sample points increases, the method should converge uniformly to the function. It turns out that this is not true in general. In particular, choosing $x_i$ to be evenly spaced and a function whose Taylor series has a finite radius of convergence this method fails spectacularly.
\begin{center}
\textcolor{red}{Add a plot here showing runge phenomena and a short description}
\end{center}

\indent We can mitigate this problem by using points which have an asymptotic distribution in $[0,1]$ given by
\begin{center}
$\rho(x) = \frac{N}{\pi \sqrt{1 - x^2}}$
\end{center}

\indent Often, we satisfy this condition using roots or extrema of certain sets of orthogonal polynomials such as Legendre or Chebyshev polynomials. These special polynomials turn out to be very useful for quadrature and derivatives, and they will be explained further in later sections.

\begin{center}
\textcolor{red}{Add a plot here showing the same function used previously but interpolated with Chebyshev nodes and a short description}
\end{center}

\subsection{Piecewise Interpolation and Splines}
\begin{center}
\textcolor{red}{Add stuff about stuff.}
\end{center}

\section{Integration}
We now want to calculate definite integrals of functions represented numerically. One way of doing this, is to decide on an interpolation scheme, and integrate the interpolant exactly. This can often be done easily when using a polynomial interpolant. What we would like is a rule as follows.

\begin{center}
$\int_{a}^{b}f(x) = \sum_{i=0}^{N-1}f(x_i)w_i$
\end{center}

The challenge is finding the right values of $w_i$.

\subsection{Newton-Cotes Formula}
The simplest method for integration is to use piecewise linear interpolation, since each region $[x_i,x_{i+1}]$ can be integrated exactly using the geometric formula for a trapezoid. The method is appropriately named the trapezoid rule.

\begin{center}
$\int_{a}^{b}f(x) = \frac{f(x_0)+f(x_1)}{2}h + \frac{f(x_1)+f(x_2)}{2}h + \dots$
\end{center}

\noindent This simplifies to the following.

\begin{center}
$\int_{a}^{b}f(x) = \frac{h}{2}(f(x_0)+f(x_N) + 2\sum_{i=1}^{N-1}f(x_i))$
\end{center}

It is easy to see using the Taylor expansion of $f(x)$ that the error in this method is $\mathcal{O}(N^{-2})$.

\begin{center}
\textcolor{red}{add a plot for this}
\end{center}

Similar rules can be derived by interpolating the function using higher order polynomials. For example, interpolating by a quadratic yields Simpson's rule. (note: N should be even)

\begin{center}
$\int_{a}^{b}f(x) = \frac{h}{3}(f(x_0)+f(x_N) + 4\sum_{i=1}^{\frac{N}{2}}f(x_{2i-1}) + 2\sum_{i=1}^{\frac{N - 1}{2}}f(x_{2i}))$
\end{center}

\subsection{Romberg Integration}
\begin{center}
\textcolor{red}{Add stuff about stuff.}
\end{center}

\subsection{Quadrature}
\begin{center}
\textcolor{red}{Add stuff about stuff.}
\end{center}

\section{Discrete Fourier Transform}
\textcolor{red}{Add stuff about stuff}

\section{Derivatives}
Now that we are able to represent functions, we want to differentiate them. This is a problem if we are representing a function as an array of sample points as the standard definition of a derivative requires taking a limit. Instead, we can decide on an interpolation scheme, and then take the derivative of the interpolant to be the derivative of our function. We can then evaluate this derivative at the same sample points to create a new array which represents the derivative.

\subsection{Finite Difference Schemes}
Consider as an example, interpolating by piecewise linear functions. Between each point $x_i$ and it's adjacent point $x_{i+1}$, we have

\begin{center}
$f(x) = f(x_i) + \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}x$
\end{center}

\noindent We can then set the derivative to

\begin{center}
$f'(x_i) = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}$
\end{center}

\noindent This can be expressed compactly as

\begin{center}
$\dv{\va{f}}{x} = \boldsymbol{D} \va{f}$
\end{center}

\noindent With $\boldsymbol{D}$ given by

\begin{center}
$
\boldsymbol{D}
=
\begin{bmatrix}
	\frac{1}{x_0 - x_1} & -\frac{1}{x_0 - x_1} & \dots & \dots & \dots \\ 
	0 & \frac{1}{x_1 - x_2} & -\frac{1}{x_1 - x_2} & \dots & \dots \\
	\vdots & 0 & \frac{1}{x_2 - x_3} & -\frac{1}{x_2 - x_3} & \dots \\
	\vdots & \vdots & \ddots & \ddots & \ddots
\end{bmatrix}
$
\end{center}

The accuracy of a method such as this depends on the choice of $x_i$'s and the particular interpolation technique. The above is an example of a finite difference scheme. In the case of equal spacing, $\boldsymbol{D}$ takes on a simple form.

\begin{center}
$
\boldsymbol{D}
= \frac{1}{h}
\begin{bmatrix}
	1 & -1 & \dots & \dots & \dots \\ 
	0 & 1 & -1 & \dots & \dots \\
	\vdots & 0 & 1 & -1 & \dots \\
	\vdots & \vdots & \ddots & \ddots & \ddots
\end{bmatrix}
$
\end{center}

\subsection{Spectral Derivatives}
\begin{center}
\textcolor{red}{Add stuff about stuff.}
\end{center}

\end{document}